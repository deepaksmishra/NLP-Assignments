{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI-CzzJw8QZl"
      },
      "source": [
        "1 Explain One-Hot Encoding\n",
        "Ans: \n",
        "One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical\n",
        " column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector.\n",
        "\n",
        "\n",
        "2 Explain Bag of Words\n",
        "Ans:The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words,\n",
        " disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision\n",
        "\n",
        "\n",
        "\n",
        "3 Explain Bag of N-Grams\n",
        "Ans: \n",
        "A bag-of-n -grams model is a way to represent a document, similar to a model. A bag-of-n -grams model represents a text document as an unordered collection of its n -grams\n",
        "\n",
        "\n",
        "4 Explain TF-IDF\n",
        "Ans: \n",
        "In information retrieval, tf–idf, TF*IDF, or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\n",
        "\n",
        "\n",
        "5 What is OOV problem?\n",
        "Ans: \n",
        "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment.\n",
        "\n",
        "6 What are word embeddings?\n",
        "Ans: \n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words \n",
        "and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems\n",
        "\n",
        "7 Explain Continuous bag of words (CBOW)\n",
        "Ans:\n",
        "Both are architectures to learn the underlying word representations for each word by using neural networks. \n",
        "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle.\n",
        "\n",
        "8 Explain SkipGram\n",
        "Ans: \n",
        "SkipGram is an algorithm that is used to create word embeddings i.e. high-dimensional vector representation of words. These embeddings are meant to encode the semantic meaning of words such that words that are semantically similar will lie close to each other in that vector's space.\n",
        "\n",
        "9 Explain Glove Embeddings.\n",
        "Ans: \n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}