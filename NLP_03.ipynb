{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEx_wYXTA2ql"
      },
      "source": [
        "Explain the basic architecture of RNN cell.\n",
        "Ans:\n",
        "All RNNs have feedback loops in the recurrent layer. This lets them maintain information in 'memory' over time. But, it can be difficult to train standard RNNs to solve problems that require learning long-term temporal dependencies. This is because the gradient of the loss function decays exponentially with time (called the vanishing gradient problem). LSTM networks are a type of RNN that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. This architecture lets them learn longer-term dependencies. GRUs are similar to LSTMs, but use a simplified structure.\n",
        " They also use a set of gates to control the flow of information, but they don't use separate memory cells, and they use fewer gates.\n",
        "\n",
        "\n",
        "Explain Backpropagation through time (BPTT)\n",
        "Ans:\n",
        "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series. A recurrent neural network is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps.\n",
        "\n",
        "\n",
        "Explain Vanishing and exploding gradients\n",
        "Ans:\n",
        "In a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large then the gradient will \n",
        "increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of exploding gradient .\n",
        "\n",
        "Explain Long short-term memory (LSTM)\n",
        "Ans:\n",
        "Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points, but also entire sequences of data. \n",
        "\n",
        "Explain Gated recurrent unit (GRU)\n",
        "Ans: \n",
        "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate.\n",
        "\n",
        "Explain Peephole LSTM\n",
        "Ans: \n",
        "Peephole connections refer to a modification to the basic LSTM architecture. ... Surprisingly, LSTM augmented by “peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps.\n",
        "\n",
        "Bidirectional RNNs\n",
        "Ans: \n",
        "Bidirectional recurrent neural networks (BRNN) connect two hidden layers of opposite directions to the same output. With this form of generative deep\n",
        "\n",
        "Explain the gates of LSTM with equations.\n",
        "Ans: \n",
        "LSTM can be used to solve problems faced by the RNN model. So, it can be used to solve:\n",
        "Long term dependency problem in RNNs.\n",
        "Vanishing Gradient & Exploding Gradient.\n",
        "The heart of a LSTM network is it’s cell or say cell state which provides a bit of memory to the LSTM so it can remember the past.\n",
        "\n",
        "Explain BiLSTM\n",
        "Ans: \n",
        "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction.\n",
        "\n",
        "\n",
        "Explain BiGRU\n",
        "Ans: \n",
        "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates.\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}