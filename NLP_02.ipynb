{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rTxc08o9GLH"
      },
      "source": [
        "1.\tWhat are Corpora?\n",
        "Ans: \n",
        "The corpora are the translations of each other. For example, a novel and its translation or a translation memory of a CAT tool could be used to build a parallel corpus.\n",
        " The user can then observe how the search word or phrase is translated.\n",
        "\n",
        "2.\tWhat are Tokens?\n",
        "Ans: \n",
        "In general, a token is an object that represents something else, such as another object (either physical or virtual), or an abstract concept as, for example, a gift is sometimes referred to as a token of the giver's esteem for the recipient.\n",
        "\n",
        "3.\tWhat are Unigrams, Bigrams, Trigrams?\n",
        "Ans: \n",
        "Using Latin numerical prefixes, an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); \n",
        "size 3 is a \"trigram\". English cardinal numbers are sometimes used, e.g., \"four-gram\", \"five-gram\", and so on.\n",
        "\n",
        "4.\tHow to generate n-grams from text?\n",
        "Ans: \n",
        "Great native python based answers given by other users. But here's the nltk approach (just in case, the OP gets penalized for reinventing what's already existing in the nltk library).\n",
        "\n",
        "There is an ngram module that people seldom use in nltk. It's not because it's hard to read ngrams, but training a model base on ngrams where n > 3 will result in much data sparsity.\n",
        "\n",
        "\n",
        "\n",
        "5.\tExplain Lemmatization\n",
        "Ans: \n",
        "Lemmatization is a linguistic term that means grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. The aim is to take away inflectional suffixes and prefixes to bring out the word's dictionary form.\n",
        "\n",
        "6.\tExplain Stemming\n",
        "Ans: Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP). ... Stemming is also a part of queries and Internet search engines.\n",
        "\n",
        "\n",
        "7.\tExplain Part-of-speech (POS) tagging\n",
        "Ans: \n",
        "In corpus linguistics, part-of-speech tagging, also called grammatical tagging is \n",
        "the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context. \n",
        "\n",
        "8.\tExplain Chunking or shallow parsing\n",
        "Ans: \n",
        "Shallow parsing (also chunking or light parsing) is an analysis of a sentence which first identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) \n",
        "and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.).\n",
        "\n",
        "9.\tExplain Noun Phrase (NP) chunking\n",
        "Ans: \n",
        "Noun phrase chunking deals with extracting the noun phrases from a sentence. While NP chunking is much simpler than parsing, it is still a challenging task to build a accurate\n",
        " and very efficient NP chunker. The importance of NP chunking derives from the fact that it is used in many applications.\n",
        "\n",
        "10.\tExplain Named Entity Recognition\n",
        "Ans: Named entity recognition is a natural language processing technique that can automatically scan entire articles and pull out some fundamental entities in a text and classify them into predefined categories.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}